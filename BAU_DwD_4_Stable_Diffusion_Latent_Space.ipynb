{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/estampa/BAU-MD-DD/blob/main/BAU_DwD_4_Stable_Diffusion_Latent_Space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv1yUcSgd27m"
      },
      "source": [
        "# Exploring the latent space of stable diffusion\n",
        "\n",
        "What's interesting is that you can generate an image from any point in latent space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:20:28.329767Z",
          "iopub.status.busy": "2024-02-21T17:20:28.329050Z",
          "iopub.status.idle": "2024-02-21T17:23:15.653382Z",
          "shell.execute_reply": "2024-02-21T17:23:15.652310Z",
          "shell.execute_reply.started": "2024-02-21T17:20:28.329734Z"
        },
        "id": "lbWtDpayDiOD",
        "trusted": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # First, let's install all the required modules.\n",
        "\n",
        "!pip install -q diffusers transformers accelerate\n",
        "!pip install -q numpy scipy ftfy Pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:23:55.606390Z",
          "iopub.status.busy": "2024-02-21T17:23:55.606005Z",
          "iopub.status.idle": "2024-02-21T17:24:12.144679Z",
          "shell.execute_reply": "2024-02-21T17:24:12.143740Z",
          "shell.execute_reply.started": "2024-02-21T17:23:55.606352Z"
        },
        "id": "gbnW1HiEDiOE",
        "trusted": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Import modules\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import time\n",
        "\n",
        "from PIL import Image\n",
        "from IPython import display as IPdisplay\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import (\n",
        "    DDIMScheduler,\n",
        "    PNDMScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    AutoencoderKL,\n",
        ")\n",
        "from transformers import logging\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "import io\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "from ipywidgets import Button\n",
        "import shutil\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "#Let's check if CUDA is available.\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# These settings are used to optimize the performance of PyTorch models on CUDA-enabled GPUs,\n",
        "# especially when using mixed precision training or inference, which can be beneficial in terms of speed and memory usage.\n",
        "# Source: https://huggingface.co/docs/diffusers/optimization/fp16#memory-efficient-attention\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:24:22.143953Z",
          "iopub.status.busy": "2024-02-21T17:24:22.143589Z",
          "iopub.status.idle": "2024-02-21T17:25:54.037631Z",
          "shell.execute_reply": "2024-02-21T17:25:54.036655Z",
          "shell.execute_reply.started": "2024-02-21T17:24:22.143923Z"
        },
        "id": "ppKz1aLSDiOF",
        "trusted": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Model\n",
        "\n",
        "#@markdown The [`stabilityai/stable-diffusion-2-1-base`](https://huggingface.co/stabilityai/stable-diffusion-2-1-base) model and the EulerDiscreteSchedulerscheduler were chosen to generate images. Despite being an older technology, it continues to enjoy popularity due to its fast performance, minimal memory requirements, and the availability of numerous community fine-tuned models built on top of SD1.5. However, you are free to experiment with other models and schedulers to compare the results.\n",
        "\n",
        "\n",
        "model_name_or_path = \"stabilityai/stable-diffusion-2-1-base\"\n",
        "\n",
        "scheduler = EulerDiscreteScheduler.from_pretrained(model_name_or_path, subfolder=\"scheduler\")\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    scheduler=scheduler,\n",
        "    torch_dtype=torch.float32,\n",
        ").to(device)\n",
        "\n",
        "# Disable image generation progress bar, we'll display our own\n",
        "pipe.set_progress_bar_config(disable=True)\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(model_name_or_path, subfolder=\"vae\", torch_dtype=torch.float16).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:25:54.040235Z",
          "iopub.status.busy": "2024-02-21T17:25:54.039388Z",
          "iopub.status.idle": "2024-02-21T17:26:00.115879Z",
          "shell.execute_reply": "2024-02-21T17:26:00.115042Z",
          "shell.execute_reply.started": "2024-02-21T17:25:54.040193Z"
        },
        "id": "1i7WuQV1DiOG",
        "trusted": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Reduce the memory consumed by the GPU.\n",
        "\n",
        "#@markdown More detailed information can be found here: https://huggingface.co/docs/diffusers/en/optimization/opt_overview\n",
        "#@markdown In particular, information about the following methods can be found here: https://huggingface.co/docs/diffusers/optimization/memory\n",
        "\n",
        "\n",
        "# Offloading the weights to the CPU and only loading them on the GPU can reduce memory consumption to less than 3GB.\n",
        "pipe.enable_model_cpu_offload()\n",
        "\n",
        "# Tighter ordering of memory tensors.\n",
        "pipe.unet.to(memory_format=torch.channels_last)\n",
        "\n",
        "# Decoding large batches of images with limited VRAM or batches with 32 images or more by decoding the batches of latents one image at a time.\n",
        "pipe.enable_vae_slicing()\n",
        "\n",
        "# Splitting the image into overlapping tiles, decoding the tiles, and then blending the outputs together to compose the final image.\n",
        "pipe.enable_vae_tiling()\n",
        "\n",
        "# Using Flash Attention; If you have PyTorch >= 2.0 installed, you should not expect a speed-up for inference when enabling xformers.\n",
        "pipe.enable_xformers_memory_efficient_attention()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:30:01.535670Z",
          "iopub.status.busy": "2024-02-21T17:30:01.535281Z",
          "iopub.status.idle": "2024-02-21T17:30:01.542928Z",
          "shell.execute_reply": "2024-02-21T17:30:01.541894Z",
          "shell.execute_reply.started": "2024-02-21T17:30:01.535637Z"
        },
        "id": "n5cKlS0CDiOG",
        "trusted": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Define functions\n",
        "\n",
        "# The path where the generated GIFs will be saved\n",
        "save_path = \"/content/output\"\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# Saves the images and a GIF\n",
        "def save_images(images, output_path):\n",
        "  # Generate a file name based on the current time, replacing colons with hyphens\n",
        "    # to ensure the filename is valid for file systems that don't allow colons.\n",
        "    dirname = (\n",
        "        time.strftime(\"%H:%M:%S\", time.localtime())\n",
        "        .replace(\":\", \"-\")\n",
        "    )\n",
        "\n",
        "    save_path = os.path.join(output_path, dirname)\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "      os.makedirs(save_path)\n",
        "\n",
        "    # Convert each image in the 'images' list from an array to an Image object.\n",
        "    converted = []\n",
        "\n",
        "    for i, image in enumerate(images):\n",
        "      pil_image = Image.fromarray(np.array(image[0], dtype=np.uint8))\n",
        "      pil_image.save(f\"{save_path}/{i:03d}.png\")\n",
        "      converted.append(pil_image)\n",
        "\n",
        "    # Save the first image in the list as a GIF file at the 'save_path' location.\n",
        "    # The rest of the images in the list are added as subsequent frames to the GIF.\n",
        "    # The GIF will play each frame for 100 milliseconds and will loop indefinitely.\n",
        "    converted[0].save(\n",
        "        f\"{save_path}/preview.gif\",\n",
        "        save_all=True,\n",
        "        append_images=converted[1:],\n",
        "        duration=100,\n",
        "        loop=0,\n",
        "    )\n",
        "\n",
        "    return save_path\n",
        "\n",
        "# Displays the GIF saved in a path\n",
        "def display_gif(path):\n",
        "    # Return the saved GIF as an IPython display object so it can be displayed in a notebook.\n",
        "\n",
        "    gif = IPdisplay.Image(f\"{path}/preview.gif\")\n",
        "\n",
        "    def download(b):\n",
        "      shutil.make_archive(path, 'zip', path)\n",
        "      files.download(path + '.zip')\n",
        "\n",
        "    button = Button(description=\"Download Images\")\n",
        "    button.on_click(download)\n",
        "\n",
        "    display(gif, button)\n",
        "\n",
        "    # return VBox([gif, button])\n",
        "\n",
        "\n",
        "# based on https://towardsdatascience.com/stable-diffusion-using-hugging-face-501d8dbdd8\n",
        "def pil_to_latents(image):\n",
        "    '''\n",
        "    Function to convert image to latents\n",
        "    '''\n",
        "    init_image = transforms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n",
        "    init_image = init_image.to(device=device, dtype=torch.float16)\n",
        "    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n",
        "    return init_latent_dist\n",
        "\n",
        "def latents_to_images(latents):\n",
        "    '''\n",
        "    Function to convert latents to images\n",
        "    '''\n",
        "    latents = (1 / 0.18215) * latents\n",
        "    with torch.no_grad():\n",
        "        image = vae.decode(latents).sample\n",
        "\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    images = (image * 255).round().astype(\"uint8\")\n",
        "    # images = ([image] for image in images)\n",
        "    return images\n",
        "\n",
        "def latents_to_pil(latents):\n",
        "    '''\n",
        "    Function to convert latents to images\n",
        "    '''\n",
        "    images = latents_to_images(latents)\n",
        "    pil_images = [Image.fromarray(image[0]) for image in images]\n",
        "    return pil_images\n",
        "\n",
        "# The function presented below stands for Spherical Linear Interpolation. It is a method\n",
        "# of interpolation on the surface of a sphere. This function is commonly used in computer\n",
        "# graphics to animate rotations in a smooth manner and can also be used to interpolate\n",
        "# between high-dimensional data points in machine learning, such as latent vectors used\n",
        "# in generative models.\n",
        "\n",
        "# The source is from Andrej Karpathy's gist: https://gist.github.com/karpathy/00103b0037c5aaea32fe1da1af553355.\n",
        "# A more detailed explanation of this method can be found at: https://en.wikipedia.org/wiki/Slerp.\n",
        "\n",
        "def slerp(v0, v1, num, t0=0, t1=1):\n",
        "    v0 = v0.detach().cpu().numpy()\n",
        "    v1 = v1.detach().cpu().numpy()\n",
        "\n",
        "    def interpolation(t, v0, v1, DOT_THRESHOLD=0.9995):\n",
        "        \"\"\"helper function to spherically interpolate two arrays v1 v2\"\"\"\n",
        "        dot = np.sum(v0 * v1 / (np.linalg.norm(v0) * np.linalg.norm(v1)))\n",
        "        if np.abs(dot) > DOT_THRESHOLD:\n",
        "            v2 = (1 - t) * v0 + t * v1\n",
        "        else:\n",
        "            theta_0 = np.arccos(dot)\n",
        "            sin_theta_0 = np.sin(theta_0)\n",
        "            theta_t = theta_0 * t\n",
        "            sin_theta_t = np.sin(theta_t)\n",
        "            s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
        "            s1 = sin_theta_t / sin_theta_0\n",
        "            v2 = s0 * v0 + s1 * v1\n",
        "        return v2\n",
        "\n",
        "    t = np.linspace(t0, t1, num)\n",
        "\n",
        "    v3 = torch.tensor(np.array([interpolation(t[i], v0, v1) for i in range(num)]))\n",
        "\n",
        "    return v3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L13Q7INNDiOG"
      },
      "source": [
        "### Generation parameters\n",
        "\n",
        "\n",
        "* `seed`: This variable is used to set a specific random seed for reproducibility.\n",
        "* `guidance_scale`: This parameter controls the extent to which the model should follow the prompt in text-to-image generation tasks, with higher values leading to stronger adherence to the prompt.       \n",
        "* `num_inference_steps`: This specifies the number of steps the model takes to generate an image. More steps can lead to a higher quality image but take longer to generate.        \n",
        "* `num_interpolation_steps`: This determines the number of steps used when interpolating between two points in the latent space, affecting the smoothness of transitions in generated       animations.        \n",
        "* `height`: The height of the generated images in pixels.       \n",
        "* `width`: The width of the generated images in pixels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm4BHESjDiOG"
      },
      "source": [
        "### Example 1: Prompt interpolation\n",
        "\n",
        "In this example, interpolation between positive and negative prompt embeddings allows exploration of space between two conceptual points defined by prompts, potentially leading to variety of images blending characteristics dictated by prompts gradually. In this case, interpolation involves adding scaled deltas to original embeddings, creating a series of new embeddings that will be used later to generate images with smooth transitions between different states based on the original prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5bJKGULCAzc"
      },
      "source": [
        "![Example 1](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/sd_interpolation_1.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:40:07.727796Z",
          "iopub.status.busy": "2024-02-21T17:40:07.727407Z",
          "iopub.status.idle": "2024-02-21T17:43:50.624205Z",
          "shell.execute_reply": "2024-02-21T17:43:50.622571Z",
          "shell.execute_reply.started": "2024-02-21T17:40:07.727768Z"
        },
        "id": "YVNrz60MDiOH",
        "trusted": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# The text prompt that describes the desired output image.\n",
        "prompt = \"Epic shot of Sweden, ultra detailed lake with an ren dear, nostalgic vintage, ultra cozy and inviting, wonderful light atmosphere, fairy, little photorealistic, digital painting, sharp focus, ultra cozy and inviting, wish to be there. very detailed, arty, should rank high on youtube for a dream trip.\" # @param {type:\"string\"}\n",
        "# A negative prompt that can be used to steer the generation away from certain features; here, it is empty.\n",
        "negative_prompt = \"poorly drawn,cartoon, 2d, disfigured, bad art, deformed, poorly drawn, extra limbs, close up, b&w, weird colors, blurry\" # @param {type:\"string\"}\n",
        "\n",
        "seed = -1 # @param {type:\"integer\"}\n",
        "guidance_scale = 8 # @param {type:\"slider\", min:0, max:20, step:0.1}\n",
        "num_inference_steps = 10 # @param {type:\"slider\", min:0, max:50, step:1}\n",
        "num_interpolation_steps = 10 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "height = 512 # @param {type:\"slider\", min:256, max:1024, step:1}\n",
        "width = 512 # @param {type:\"slider\", min:256, max:1024, step:1}\n",
        "\n",
        "if seed >= 0:\n",
        "    generator = torch.manual_seed(seed)\n",
        "    print( \"seed:\", seed)\n",
        "else:\n",
        "    generator = None\n",
        "    print( \"seed:\", torch.seed() )\n",
        "\n",
        "\n",
        "# The step size for the interpolation in the latent space.\n",
        "step_size = 0.001\n",
        "\n",
        "# First of all, we need to tokenize and obtain embeddings for both positive and negative text prompts. The positive prompt guides the image generation towards the desired characteristics, while the negative prompt steers it away from unwanted features.\n",
        "\n",
        "# Tokenizing and encoding the prompt into embeddings.\n",
        "prompt_tokens = pipe.tokenizer(\n",
        "    prompt,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "prompt_embeds = pipe.text_encoder(prompt_tokens.input_ids.to(device))[0]\n",
        "\n",
        "\n",
        "# Tokenizing and encoding the negative prompt into embeddings.\n",
        "if negative_prompt is None:\n",
        "    negative_prompt = [\"\"]\n",
        "\n",
        "negative_prompt_tokens = pipe.tokenizer(\n",
        "    negative_prompt,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "negative_prompt_embeds = pipe.text_encoder(negative_prompt_tokens.input_ids.to(device))[0]\n",
        "\n",
        "# Now let's look at the code part that generates a random initial vector using a normal distribution\n",
        "# that is structured to match the dimensions expected by the diffusion model (UNet). This allows for\n",
        "# the reproducibility of the results by optionally using a random number generator. After creating\n",
        "# the initial vector, the code performs a series of interpolations between the two embeddings\n",
        "# (positive and negative prompts), by incrementally adding a small step size for each iteration.\n",
        "# The results are stored in a list named \"walked_embeddings\".\n",
        "\n",
        "# Generating initial latent vectors from a random normal distribution, with the option to use a generator for reproducibility.\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        ")\n",
        "\n",
        "walked_embeddings = []\n",
        "\n",
        "# Interpolating between embeddings for the given number of interpolation steps.\n",
        "for i in range(num_interpolation_steps):\n",
        "    walked_embeddings.append(\n",
        "        [prompt_embeds + step_size * i, negative_prompt_embeds + step_size * i]\n",
        "    )\n",
        "\n",
        "\n",
        "# Finally, let's generate a series of images based on interpolated embeddings and then displaying\n",
        "# these images. We'll iterate over an array of embeddings, using each to generate an image with\n",
        "# specified characteristics like height, width, and other parameters relevant to image generation.\n",
        "# Then we'll collect these images into a list. Once generation is complete we'll call the\n",
        "# `display_image` function to save and display these images as GIF at a given save path.\n",
        "\n",
        "# Generating images using the interpolated embeddings.\n",
        "images = []\n",
        "for latent in tqdm(walked_embeddings):\n",
        "    images.append(\n",
        "        pipe(\n",
        "            height=height,\n",
        "            width=width,\n",
        "            num_images_per_prompt=1,\n",
        "            prompt_embeds=latent[0],\n",
        "            negative_prompt_embeds=latent[1],\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=generator,\n",
        "            latents=latents,\n",
        "        ).images\n",
        "    )\n",
        "\n",
        "# Display of saved generated images.\n",
        "path = save_images(images, save_path)\n",
        "display_gif(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZQWop9nDiOH"
      },
      "source": [
        "### Example 2: Diffusion latents interpolation for a single prompt\n",
        "Unlike the first example, in this one, we are performing interpolation between the two embeddings of the diffusion model itself, not the prompts. Please note that in this case, we use the slerp function for interpolation. However, there is nothing stopping us from adding a constant value to one embedding instead.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-tYO2rMCAzd"
      },
      "source": [
        "![Example 2](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/sd_interpolation_2.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grgP7UNpDiOH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# The text prompt that describes the desired output image.\n",
        "prompt = \"\" # @param {type:\"string\"}\n",
        "# A negative prompt that can be used to steer the generation away from certain features; here, it is empty.\n",
        "negative_prompt = \"\" # @param {type:\"string\"}\n",
        "\n",
        "seed = -1 # @param {type:\"integer\"}\n",
        "guidance_scale = 8 # @param {type:\"slider\", min:0, max:20, step:0.1}\n",
        "num_inference_steps = 10 # @param {type:\"slider\", min:0, max:50, step:1}\n",
        "num_interpolation_steps = 10 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "height = 512 # @param {type:\"slider\", min:256, max:1024, step:1}\n",
        "width = 512 # @param {type:\"slider\", min:256, max:1024, step:1}\n",
        "\n",
        "if seed >= 0:\n",
        "    generator = torch.manual_seed(seed)\n",
        "    print( \"seed:\", seed)\n",
        "else:\n",
        "    generator = None\n",
        "    print( \"seed:\", torch.seed() )\n",
        "\n",
        "# Generating initial latent vectors from a random normal distribution. In this example two latent vectors are generated, which will serve as start and end points for the interpolation.\n",
        "# These vectors are shaped to fit the input requirements of the diffusion model's U-Net architecture.\n",
        "latents = torch.randn(\n",
        "    (2, pipe.unet.config.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        ")\n",
        "\n",
        "# Getting our latent embeddings\n",
        "interpolated_latents = slerp(latents[0], latents[1], num_interpolation_steps)\n",
        "\n",
        "# Generating images using the interpolated embeddings.\n",
        "images = []\n",
        "for latent_vector in tqdm(interpolated_latents):\n",
        "    images.append(\n",
        "        pipe(\n",
        "            prompt,\n",
        "            height=height,\n",
        "            width=width,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_images_per_prompt=1,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=generator,\n",
        "            latents=latent_vector[None, ...],\n",
        "        ).images\n",
        "    )\n",
        "\n",
        "# Display of saved generated images.\n",
        "path = save_images(images, save_path)\n",
        "display_gif(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTFrAlwrDiOI"
      },
      "source": [
        "### Example 3: Interpolation between multiple prompts\n",
        "\n",
        "In contrast to the first example, where we moved away from a single prompt, in this example, we will be interpolating between any number of prompts. To do so, we will take consecutive pairs of prompts and create smooth transitions between them. Then, we will combine the interpolations of these consecutive pairs, and instruct the model to generate images based on them. For interpolation we will use the slerp function, as in the second example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QFtzMlOCAzd"
      },
      "source": [
        "![Example 3](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/sd_interpolation_3.gif)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Specify a list of prompts (a prompt per line)\n",
        "\n",
        "from ipywidgets import Textarea, HBox\n",
        "\n",
        "try:\n",
        "  value=list1.value\n",
        "except:\n",
        "  value=None\n",
        "\n",
        "list1 = Textarea(\n",
        "    value=value,\n",
        "    layout={'width': '45%', 'height':'95%'},\n",
        "    description='Prompts')\n",
        "\n",
        "try:\n",
        "  value=list2.value\n",
        "except:\n",
        "  value=None\n",
        "\n",
        "list2 = Textarea(\n",
        "    value=value,\n",
        "    layout={'width': '45%', 'height':'95%'},\n",
        "    description='Negative<br/>prompts')\n",
        "\n",
        "display(HBox([list1,list2], layout={'height':'250px'}))"
      ],
      "metadata": {
        "id": "j1ZSbG2v2hUN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_NEQjG_CAze",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "seed = -1 # @param {type:\"integer\"}\n",
        "guidance_scale = 8 # @param {type:\"slider\", min:0, max:20, step:0.1}\n",
        "num_inference_steps = 10 # @param {type:\"slider\", min:0, max:50, step:1}\n",
        "num_interpolation_steps = 10 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "height = 512 # @param {type:\"slider\", min:256, max:1024, step:1}\n",
        "width = 512 # @param {type:\"slider\", min:256, max:1024, step:1}\n",
        "\n",
        "if seed >= 0:\n",
        "    generator = torch.manual_seed(seed)\n",
        "    print( \"seed:\", seed)\n",
        "else:\n",
        "    generator = None\n",
        "    print( \"seed:\", torch.seed() )\n",
        "\n",
        "\n",
        "# Once again, let's tokenize and obtain embeddings but this time for multiple positive and negative text prompts.\n",
        "\n",
        "prompts = list1.value.split('\\n')\n",
        "negative_prompts = list2.value.split('\\n')\n",
        "\n",
        "batch_size = len(prompts)\n",
        "\n",
        "if len(negative_prompts) < batch_size:\n",
        "  negative_prompts += [''] * (batch_size - len(negative_prompts))\n",
        "else:\n",
        "  negative_prompts = negative_prompts[:batch_size]\n",
        "\n",
        "# Tokenizing and encoding prompts into embeddings.\n",
        "prompts_tokens = pipe.tokenizer(\n",
        "    prompts,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "prompts_embeds = pipe.text_encoder(\n",
        "    prompts_tokens.input_ids.to(device)\n",
        ")[0]\n",
        "\n",
        "negative_prompts_tokens = pipe.tokenizer(\n",
        "    negative_prompts,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "negative_prompts_embeds = pipe.text_encoder(\n",
        "    negative_prompts_tokens.input_ids.to(device)\n",
        ")[0]\n",
        "\n",
        "\n",
        "# We will take consecutive pairs of prompts and create smooth transitions between them with `slerp` function.\n",
        "\n",
        "# Generating initial U-Net latent vectors from a random normal distribution.\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        ")\n",
        "\n",
        "# Interpolating between embeddings pairs for the given number of interpolation steps.\n",
        "interpolated_prompt_embeds = []\n",
        "interpolated_negative_prompts_embeds = []\n",
        "for i in range(batch_size - 1):\n",
        "    interpolated_prompt_embeds.append(\n",
        "        slerp(\n",
        "            prompts_embeds[i],\n",
        "            prompts_embeds[i + 1],\n",
        "            num_interpolation_steps\n",
        "        )\n",
        "    )\n",
        "    interpolated_negative_prompts_embeds.append(\n",
        "        slerp(\n",
        "            negative_prompts_embeds[i],\n",
        "            negative_prompts_embeds[i + 1],\n",
        "            num_interpolation_steps,\n",
        "        )\n",
        "    )\n",
        "\n",
        "interpolated_prompt_embeds = torch.cat(\n",
        "    interpolated_prompt_embeds, dim=0\n",
        ").to(device)\n",
        "\n",
        "interpolated_negative_prompts_embeds = torch.cat(\n",
        "    interpolated_negative_prompts_embeds, dim=0\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# Finally, we need to generate images based on the embeddings.\n",
        "\n",
        "# Generating images using the interpolated embeddings.\n",
        "images = []\n",
        "for prompt_embeds, negative_prompt_embeds in tqdm(\n",
        "    zip(interpolated_prompt_embeds, interpolated_negative_prompts_embeds),\n",
        "    total=len(interpolated_prompt_embeds),\n",
        "):\n",
        "    images.append(\n",
        "        pipe(\n",
        "            height=height,\n",
        "            width=width,\n",
        "            num_images_per_prompt=1,\n",
        "            prompt_embeds=prompt_embeds[None, ...],\n",
        "            negative_prompt_embeds=negative_prompt_embeds[None, ...],\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=generator,\n",
        "            latents=latents,\n",
        "        ).images\n",
        "    )\n",
        "\n",
        "# Display of saved generated images.\n",
        "path = save_images(images, save_path)\n",
        "display_gif(path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQqANSP2DiOI"
      },
      "source": [
        "### Example 4: Circular walk through the diffusion latent space for a single prompt\n",
        "\n",
        "This example was taken from: https://keras.io/examples/generative/random_walks_with_stable_diffusion/       \n",
        "\n",
        "Let's imagine that we have two noise components, which we'll call x and y. We start by moving from 0 to 2π and at each step we add the cosine of x and the sine of y to the result. Using this approach, at the end of our movement we end up with the same noise values ​​that we started with. This means that vectors end up turning into themselves, ending our movement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn70u0UqCAzi"
      },
      "source": [
        "![Example 4](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/sd_interpolation_4.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac-68CTWDiOJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# The text prompt that describes the desired output image.\n",
        "prompt = \"\" # @param {type:\"string\"}\n",
        "# A negative prompt that can be used to steer the generation away from certain features; here, it is empty.\n",
        "negative_prompt = \"\" # @param {type:\"string\"}\n",
        "\n",
        "seed = -1 # @param {type:\"integer\"}\n",
        "guidance_scale = 8 # @param {type:\"slider\", min:0, max:20, step:0.1}\n",
        "num_inference_steps = 10 # @param {type:\"slider\", min:0, max:50, step:1}\n",
        "num_interpolation_steps = 10 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "height = 512 # @param {type:\"slider\", min:256, max:1024, step:1}\n",
        "width = 512 # @param {type:\"slider\", min:256, max:1024, step:1}\n",
        "\n",
        "if seed >= 0:\n",
        "    generator = torch.manual_seed(seed)\n",
        "    print( \"seed:\", seed)\n",
        "else:\n",
        "    generator = None\n",
        "    print( \"seed:\", torch.seed() )\n",
        "\n",
        "# Generating initial latent vectors from a random normal distribution to create a loop interpolation between them.\n",
        "latents = torch.randn(\n",
        "    (2, 1, pipe.unet.config.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        ")\n",
        "\n",
        "\n",
        "# Calculation of looped embeddings\n",
        "walk_noise_x = latents[0].to(device)\n",
        "walk_noise_y = latents[1].to(device)\n",
        "\n",
        "# Walking on a trigonometric circle\n",
        "walk_scale_x = torch.cos(torch.linspace(0, 2, num_interpolation_steps) * np.pi).to(\n",
        "    device\n",
        ")\n",
        "walk_scale_y = torch.sin(torch.linspace(0, 2, num_interpolation_steps) * np.pi).to(\n",
        "    device\n",
        ")\n",
        "\n",
        "# Applying interpolation to noise\n",
        "noise_x = torch.tensordot(walk_scale_x, walk_noise_x, dims=0)\n",
        "noise_y = torch.tensordot(walk_scale_y, walk_noise_y, dims=0)\n",
        "\n",
        "circular_latents = noise_x + noise_y\n",
        "\n",
        "# Generating images using the interpolated embeddings.\n",
        "images = []\n",
        "for latent_vector in tqdm(circular_latents):\n",
        "    images.append(\n",
        "        pipe(\n",
        "            prompt,\n",
        "            height=height,\n",
        "            width=width,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_images_per_prompt=1,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=generator,\n",
        "            latents=latent_vector,\n",
        "        ).images\n",
        "    )\n",
        "\n",
        "# Display of saved generated images.\n",
        "path = save_images(images, save_path)\n",
        "display_gif(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tgbkOxbkf8B"
      },
      "source": [
        "# Finalizing\n",
        "\n",
        "When you finish working you have to remember to **stop the runtime**, because there is a time limit and to avoid wasting resources. To stop the runtime click Manage Sessions on the Runtime menu. Once the dialog opens click terminate on the current runtime.\n",
        "\n",
        "> But when you stop the runtime everything you have not saved is ⚠ **lost** ⚠, so be sure to **download** everything you want to keep before stopping it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNi8VV-wJDFi"
      },
      "source": [
        "# Credits\n",
        "\n",
        "Modified from https://huggingface.co/learn/cookbook/en/stable_diffusion_interpolation by Taller Estampa https://tallerestampa.com / https://github.com/estampa"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}