{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BAU-MD-DD_03_CLIP",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Seleccionar im√°genes mediante CLIP\n",
        "\n"
      ],
      "metadata": {
        "id": "VK4CKbzBycBe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eK5585Ti0jT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ‚ñ∂ Instalamos las librer√≠as necesarias\n",
        "\n",
        "! pip install -q ftfy regex tqdm mediapy\n",
        "! pip install -q git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚ñ∂ Conectanos el drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4B2ZO_gVpHSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos las funciones necesarias"
      ],
      "metadata": {
        "id": "LJlRY1reyvMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚ñ∂ Definimos las funciones necesarias\n",
        "\n",
        "from clip import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import mediapy as media\n",
        "\n",
        "def precompute(model, preprocess, device, files):\n",
        "    # You can try tuning the batch size for very large videos, but it should usually be OK\n",
        "    batch_size = 256\n",
        "    batches = math.ceil(len(files) / batch_size)\n",
        "\n",
        "    # The encoded features will bs stored in images_features\n",
        "    images_features = torch.empty([0, 512], dtype=torch.float16).to(device)\n",
        "\n",
        "    # Process each batch\n",
        "    for i in range(batches):\n",
        "        print(f\"Processing batch {i + 1}/{batches}\")\n",
        "\n",
        "        # Get the relevant frames\n",
        "        batch_files = files[i * batch_size: (i + 1) * batch_size]\n",
        "\n",
        "        # Load the images for the batch\n",
        "        images = load_images(batch_files)\n",
        "        # Preprocess the images for the batch\n",
        "        batch_preprocessed = torch.stack([preprocess(image) for image in images]).to(device)\n",
        "\n",
        "        # Encode with CLIP and normalize\n",
        "        with torch.no_grad():\n",
        "            batch_features = model.encode_image(batch_preprocessed)\n",
        "            batch_features /= batch_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Append the batch to the list containing all features\n",
        "        images_features = torch.cat((images_features, batch_features))\n",
        "\n",
        "    return images_features\n",
        "\n",
        "\n",
        "def load_images(files):\n",
        "    images = []\n",
        "    for file in files:\n",
        "        image = Image.open(file)\n",
        "        images.append(image)\n",
        "        # images.append(np.asarray(image))\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def find_closest_text(model, device, files, image_features, text, num):\n",
        "    # Encode and normalize the search query using CLIP\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(clip.tokenize(text).to(device))\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Compute the similarity between the search query and each frame using the Cosine similarity\n",
        "    similarities = (100.0 * image_features @ text_features.T.cpu().numpy()).flatten()\n",
        "    # Get the nth most similar\n",
        "    ind = np.argpartition(similarities, -num)[-num:]\n",
        "    ind_sorted = ind[np.argsort(similarities[ind]*-1)]\n",
        "    return np.array(files)[ind_sorted]\n",
        "\n",
        "\n",
        "def find(folder, description, num=10):\n",
        "    files = glob.glob(os.path.join(folder, '*.jpg'))\n",
        "\n",
        "    # Load the open CLIP model\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "    if os.path.isfile(os.path.join(folder, 'image_features.npz')):\n",
        "        image_features = np.load(os.path.join(folder, 'image_features.npz'))[\"arr_0\"]\n",
        "    else:\n",
        "        image_features = precompute(model, preprocess, device, files).cpu()\n",
        "        np.savez_compressed(os.path.join(folder, 'image_features.npz'), image_features)\n",
        "\n",
        "    return find_closest_text(model, device, files, image_features, description, num)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TQmGyuKlpbJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚ñ∂ Buscamos las im√°genes que m√°s se adecuan a una descripci√≥n\n",
        "\n",
        "#@markdown La primera vez procesar√° todas las im√°genes, as√≠ que tardar√° un poco,\n",
        "#@markdown en funci√≥n del n√∫mero de im√°genes. Generar√° un fichero llamado \n",
        "#@markdown image-features.npz en el directorio de la im√°genes que usar√° en \n",
        "#@markdown b√∫squedas futuras. Si se a√±aden o quitan im√°genes del directorio \n",
        "#@markdown hay que borrar este fichero para que lo recalcule.\n",
        "\n",
        "#@markdown üí¨ Directorio de las im√°genes (en formato jpg)\n",
        "input_folder = \"/content/drive/MyDrive/BAU-DD/person\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown üí¨ Descripci√≥n con la que buscar\n",
        "input_description = \"a skilled worker\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown üí¨ N√∫mero de im√°genes a seleccionar\n",
        "input_number = 10#@param {type:\"integer\"}\n",
        "\n",
        "selected_files = find(input_folder, input_description, input_number)\n",
        "images = [Image.open(file) for file in selected_files]\n",
        "filenames = [os.path.basename(file) for file in selected_files]\n",
        "media.show_images(images, border=True, height=256, columns=4, titles=filenames)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vFyFJ0ifqFIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚ñ∂ Descargamos la selecci√≥n\n",
        "\n",
        "safe_filename = \"\".join([c for c in input_description if c.isalpha() or c.isdigit() or c==' ']).rstrip().replace(\" \", \"_\")\n",
        "file_list = \" \".join(selected_files)\n",
        "\n",
        "!rm \"/tmp/{safe_filename}.zip\"\n",
        "!zip -qjr -0 /tmp/{safe_filename}.zip $file_list\n",
        "\n",
        "from google.colab import files\n",
        "files.download(os.path.join('/tmp', safe_filename + \".zip\"))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wkJOBrb-0qCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNi8VV-wJDFi"
      },
      "source": [
        "# Credits\n",
        "\n",
        "Taller Estampa https://tallerestampa.com / https://github.com/estampa\n",
        "\n",
        "### Based on\n",
        "\n",
        "[Clip](https://github.com/openai/CLIP)\n"
      ]
    }
  ]
}