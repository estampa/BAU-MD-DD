{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-PBOMwsyfwnb"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introductory notebook with colab basics and a simple neural network example\n",
        "\n",
        "## Outline\n",
        "\n",
        "1.   Notebook saving\n",
        "2.   Colab basic concepts\n",
        "3.   Running a neural network\n",
        "4.   Google drive integration and automation"
      ],
      "metadata": {
        "id": "570NYcGS-4mB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JeP7k82oCKG"
      },
      "source": [
        "This a Colaboratory notebook, a document containing information and executable code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aro-UJgUQSH1"
      },
      "source": [
        "## Integration with Drive\n",
        "\n",
        "Colaboratory is integrated with Google Drive. It allows you to share, comment, and collaborate on the same document with multiple people:\n",
        "\n",
        "* **File->Make a Copy** creates a copy of the notebook in Drive.\n",
        "\n",
        "* **File->Save** saves the File to Drive. **File->Save and checkpoint** pins the version so it doesn't get deleted from the revision history.\n",
        "\n",
        "* **File->Revision history** shows the notebook's revision history.\n",
        "\n",
        "* The **Share** button (top-right of the toolbar) allows you to share the notebook and control permissions set on it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR921S_OQSHG"
      },
      "source": [
        "## Runtime\n",
        "To execute the notebook, you must connect to a Runtime by clicking connect on the upper right corner.\n",
        "\n",
        "This runtime *is* a server in a datacenter somewhere. So everything that you do **here** is really done **there**.\n",
        "\n",
        "**When you stop the runtime everything you have not saved is lost.**\n",
        "\n",
        "When you finish working you have to remember to stop the runtime, because there is a time limit and to avoid wasting resources. To stop the runtime click **Manage Sessions** on the **Runtime** menu. Once the dialog opens click terminate on the current runtime. It's something like shutting down the computer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaXbrlXGn7Ta"
      },
      "source": [
        "## Files\n",
        "\n",
        "Since everything that we do in colab is done on a remote server, all the files are also stored there. There is a simple file browser on the left column, it can be opened clicking in the ðŸ“ icon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyG45Qk3qQLS"
      },
      "source": [
        "## Cells\n",
        "A notebook is a list of cells. Cells contain either explanatory text or executable code and its output. Click a cell to select it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVH4ONSoO0jf"
      },
      "source": [
        "### Code cells\n",
        "Below there's a **code cell**. Once you have connected to a Runtime click in the cell to select it and execute the contents clicking the **Play icon** in the left gutter of the cell.\n",
        "\n",
        "Once the cell has executed a green checkbox icon will be shown on the left.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "fIgjUg0xOeh5"
      },
      "source": [
        "10 + 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code cells can run Python code like before or execute system commands on the remote machine when preceeded with ! or %.\n",
        "\n",
        "In the next cell we list the contents of the current folder."
      ],
      "metadata": {
        "id": "9qeViaP-J4Pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "DLkF4AcqKLFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv1yUcSgd27m"
      },
      "source": [
        "# Running computer vision neural networks using a library and automating it.\n",
        "\n",
        "We will see how to easily run computer vision neural networks writing code using a library. Then we will see how to automate the preocessing.\n",
        "\n",
        "## Outline\n",
        "\n",
        "1.   Simple way to run different neural networks for different tasks\n",
        "  1.   Huggingface\n",
        "  2.   Specific libraries (Yolo)\n",
        "2.   Google Drive\n",
        "3.   Creating a GUI and automating computer vision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we install the library."
      ],
      "metadata": {
        "id": "yxqDfdomsA6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!pip install -q transformers gradio ultralytics"
      ],
      "metadata": {
        "id": "SvOI4TDZJMKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Huggingface"
      ],
      "metadata": {
        "id": "0Cv2G1iQMoEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This library can run different tasks, that they organize in what they call [pipelines](https://huggingface.co/docs/transformers/pipeline_tutorial), and they are based on tasks or input/output format, like DepthEstimation or TextToAudio ([list](https://huggingface.co/docs/transformers/main_classes/pipelines)).\n",
        "\n",
        "We can create a pipeline with the task name\n",
        "\n",
        "```\n",
        "transcriber = pipeline(task=\"automatic-speech-recognition\")\n",
        "```\n",
        "or by selecting the model\n",
        "```\n",
        "transcriber = pipeline(model=\"openai/whisper-large-v2\")\n",
        "```"
      ],
      "metadata": {
        "id": "hI1KV6BjKhkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "captioner = pipeline(model=\"Salesforce/blip-image-captioning-base\", device=0)"
      ],
      "metadata": {
        "id": "93AW3QCgKg3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once created we can give it an input and it will process it and give us an output."
      ],
      "metadata": {
        "id": "TQwX9i8-MJAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = captioner(\"https://farm4.staticflickr.com/3129/3189318645_5466feb31a_z.jpg\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "AQTL7AAoNAq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create other pipelines for other tasks. In this case we are going to use a DepthEstimate pipeline."
      ],
      "metadata": {
        "id": "AjPq66_8MwwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depth_estimator = pipeline(task=\"depth-estimation\", model=\"Intel/dpt-hybrid-midas\", device=0)\n",
        "output = depth_estimator(\"https://farm4.staticflickr.com/3129/3189318645_5466feb31a_z.jpg\")\n",
        "display(output[\"depth\"])"
      ],
      "metadata": {
        "id": "qY4WrBXkizt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PBOMwsyfwnb"
      },
      "source": [
        "## Initialize another library (Yolo) for other computer vision tasks\n",
        "\n",
        "First we load the models.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "yolo_detection = YOLO('yolov8m.pt')  # load a model for object detection\n",
        "yolo_pose = YOLO('yolov8m-pose.pt')  # load a model for pose detection\n",
        "yolo_seg = YOLO('yolov8m-seg.pt')    # load a model for segmentation"
      ],
      "metadata": {
        "id": "rPSdxyxAgNu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we define the function to process the images"
      ],
      "metadata": {
        "id": "dBhrjeuHxwAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_detections(image):\n",
        "  \"\"\"\n",
        "  This function applies object detection to the image\n",
        "\n",
        "  Args:\n",
        "    image (PIL.Image): The input image.\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing the image with detections plotted and the JSON representation of the detections.\n",
        "  \"\"\"\n",
        "  detections = yolo_detection(image)\n",
        "  detection_image = detections[0].plot(img=Image.new('RGB', (detections[0].orig_shape[1], detections[0].orig_shape[0])))\n",
        "  return detection_image, detections[0].tojson()\n",
        "\n",
        "def process_pose(image):\n",
        "  \"\"\"\n",
        "  This function applies pose detection to the image\n",
        "\n",
        "  Args:\n",
        "    image (PIL.Image): The input image.\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing the image with pose plotted and the JSON representation of the pose.\n",
        "  \"\"\"\n",
        "  pose = yolo_pose(image)\n",
        "  pose_image = pose[0].plot(boxes=False, labels=False, img=Image.new('RGB', (pose[0].orig_shape[1], pose[0].orig_shape[0])))\n",
        "  return pose_image, pose[0].tojson()\n",
        "\n",
        "def process_seg(image):\n",
        "  \"\"\"\n",
        "  This function applies segmentation to the image\n",
        "\n",
        "  Args:\n",
        "    image (PIL.Image): The input image.\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing the image with segmentation plotted and the JSON representation of the segmentation.\n",
        "  \"\"\"\n",
        "  seg = yolo_seg(image)\n",
        "  seg_image = seg[0].plot(boxes=False, labels=False, img=Image.new('RGB', (seg[0].orig_shape[1], seg[0].orig_shape[0])))\n",
        "  return seg_image, seg[0].tojson()"
      ],
      "metadata": {
        "id": "4pZfeMetxvLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process an image with the different models"
      ],
      "metadata": {
        "id": "0HMhmUiFriH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "# Load an image\n",
        "url = \"https://farm4.staticflickr.com/3129/3189318645_5466feb31a_z.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# Apply object detection\n",
        "detection_image, detection_json = process_detections(image)\n",
        "display(detection_image)\n",
        "\n",
        "# Apply pose detection\n",
        "pose_image, pose_json = process_pose(image)\n",
        "display(pose_image)\n",
        "\n",
        "# Apply segmentation\n",
        "seg_image, seg_json = process_seg(image)\n",
        "display(seg_image)"
      ],
      "metadata": {
        "id": "kC43HcDChNH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THnqWirZNRUG"
      },
      "source": [
        "## Google drive\n",
        "\n",
        "Optional: connect to google drive if you want to use images from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqmgiToaOQz3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Give it a GUI\n",
        "\n",
        "We can also create a GUI in Colab.\n"
      ],
      "metadata": {
        "id": "1GWqW84CFcEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown  â–¶  We first define some processing functions\n",
        "\n",
        "#@markdown This cell has the code hidden, double click to view it.\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "\n",
        "def process_caption(image):\n",
        "  output = captioner(image)\n",
        "  return None, output\n",
        "\n",
        "def process_depth(image):\n",
        "  output = depth_estimator(image)\n",
        "  return output[\"depth\"], None\n",
        "\n",
        "def process_single(image, task):\n",
        "  \"\"\"\n",
        "  This function processes the given image based on the specified task.\n",
        "\n",
        "  Args:\n",
        "    image (PIL.Image): The input image.\n",
        "    task (str): The task to be performed. It can be \"detection\", \"segmentation\", \"pose\", \"depth\" or \"caption\".\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing the image with the task results plotted and the JSON representation of the results.\n",
        "    If the task is not recognized, it defaults to processing depth.\n",
        "  \"\"\"\n",
        "  if task == \"detection\":\n",
        "    return process_detections(image)\n",
        "  elif task == \"segmentation\":\n",
        "    return process_seg(image)\n",
        "  elif task == \"pose\":\n",
        "    return process_pose(image)\n",
        "  elif task == \"depth\":\n",
        "    return process_depth(image)\n",
        "  elif task == \"caption\":\n",
        "    return process_caption(image)\n",
        "\n",
        "def process(uploaded_files, local_files, output_folder, task):\n",
        "    \"\"\"\n",
        "    This function processes the images selected in the GUI\n",
        "\n",
        "    Args:\n",
        "      uploaded_files: files uploaded through the GUI\n",
        "      local_files: files selected from the local file system\n",
        "      output_folder: the folder where the results will be saved\n",
        "      task (str): The task to be performed. It can be \"detection\", \"segmentation\", \"pose\", or \"depth\".\n",
        "\n",
        "    Returns:\n",
        "      tuple: A tuple of lists containing the processed images and the corresponding JSON files\n",
        "    \"\"\"\n",
        "\n",
        "    # Check the input\n",
        "    if len(output_folder) == 0:\n",
        "        raise gr.Error(\"You have to select an output folder!\")\n",
        "\n",
        "    if uploaded_files is None and len(local_files) == 0:\n",
        "        raise gr.Error(\"You have to select at least one file or folder!\")\n",
        "\n",
        "    output_folder = os.path.dirname(output_folder[0]) if not os.path.isdir(output_folder[0]) else output_folder[0]\n",
        "\n",
        "    input_files = []\n",
        "    output_images = []\n",
        "    output_json = []\n",
        "\n",
        "    # List all the files to be processed\n",
        "    if uploaded_files is not None:\n",
        "        if isinstance(uploaded_files, list):\n",
        "            input_files.extend(uploaded_files)\n",
        "        else:\n",
        "            input_files.append(uploaded_files)\n",
        "\n",
        "    input_files.extend(local_files)\n",
        "\n",
        "    input_files = [input_file for input_file in input_files if (input_file.lower().endswith(\".jpg\") or input_file.lower().endswith(\".jpeg\") or input_file.lower().endswith(\".png\")) and not os.path.isdir(input_file)]\n",
        "\n",
        "    # Process all the files\n",
        "    if len(input_files) > 0:\n",
        "        for input_file in input_files:\n",
        "            # Open the image\n",
        "            image = Image.open(input_file)\n",
        "            image = ImageOps.exif_transpose(image)\n",
        "\n",
        "            # Process the image\n",
        "            result_image, result_json = process_single(image, task)\n",
        "            output_json.append(result_json)\n",
        "\n",
        "            # Save the resulting image\n",
        "            if result_image is not None:\n",
        "              output_images.append(result_image)\n",
        "              output_filename = f\"{Path(input_file).stem}-{task}.png\"\n",
        "              output_path = os.path.join(output_folder, output_filename)\n",
        "              if not isinstance(result_image, Image.Image):\n",
        "                  result_image = Image.fromarray(result_image)\n",
        "              result_image.save(output_path)\n",
        "\n",
        "            yield output_images, output_json"
      ],
      "metadata": {
        "cellView": "form",
        "id": "n-WCKHGey3qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown  â–¶  Then we create the GUI\n",
        "\n",
        "import os\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  with gr.Column():\n",
        "    input_file = gr.File(file_count=\"multiple\", file_types=[\".jpg\", \".png\"], label=\"Input images\")\n",
        "    input_files = gr.FileExplorer(label=\"Remote files\")\n",
        "    output_folder = gr.FileExplorer(label=\"Remote output folder\")\n",
        "    task = gr.Radio([\"detection\", \"segmentation\", \"pose\", \"depth\", \"caption\"], label=\"Task\", value=\"detection\")\n",
        "    process_button = gr.Button(value=\"Process\")\n",
        "\n",
        "  with gr.Column():\n",
        "    with gr.Tabs():\n",
        "      with gr.Tab(label=\"Images\"):\n",
        "        gallery = gr.Gallery(label=\"Processed images\", show_label=False, columns=[3], object_fit=\"contain\", height=\"auto\")\n",
        "      with gr.Tab(label=\"Data\"):\n",
        "        json = gr.JSON(label=\"Output data\")\n",
        "\n",
        "  process_button.click(process, [input_file, input_files, output_folder, task], [gallery, json])\n",
        "\n",
        "demo.launch(quiet=True, debug=False, height=768)\n"
      ],
      "metadata": {
        "id": "pjfFvGfWFvC5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tgbkOxbkf8B"
      },
      "source": [
        "# Finalizing\n",
        "\n",
        "When you finish working you have to remember to **stop the runtime**, because there is a time limit and to avoid wasting resources. To stop the runtime click Manage Sessions on the Runtime menu. Once the dialog opens click terminate on the current runtime.\n",
        "\n",
        "> But when you stop the runtime everything you have not saved is âš  **lost** âš , so be sure to **download** everything you want to keep before stopping it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNi8VV-wJDFi"
      },
      "source": [
        "# Credits\n",
        "\n",
        "Taller Estampa https://tallerestampa.com / https://github.com/estampa\n"
      ]
    }
  ]
}